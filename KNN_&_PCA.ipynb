{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MODULE: 11 (KNN & PCA)\n",
        "#ASSIGNMENT CODE: DA-AG-016"
      ],
      "metadata": {
        "id": "9iGk_luxOYb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "ANS:\n",
        "\n",
        "#What Is K-Nearest Neighbors (KNN)?\n",
        "\n",
        "> Non-parametric, supervised learning technique: KNN doesn’t assume a predetermined form for the data distribution—it learns directly from the dataset.\n",
        "\n",
        "> Originated with Evelyn Fix and Joseph Hodges in 1951; Thomas Cover later expanded the idea.\n",
        "\n",
        "> Also known as a lazy learner: instead of training a model upfront, it stores the entire training set and performs computation only when making predictions.\n",
        "\n",
        "#How KNN Works?\n",
        "\n",
        "> Step 1: Compute Distances\n",
        "\n",
        "For a new input, compute its distance to all training points. Common metrics include:\n",
        "\n",
        ">> Euclidean distance (most common for continuous data)\n",
        "\n",
        ">> Manhattan or Minkowski distance, Hamming, Pearson/Spearman, depending on data type.\n",
        "\n",
        "> Step 2: Identify the “K” Nearest Neighbors\n",
        "\n",
        "Select the K training points that are closest to the new point.\n",
        "\n",
        "> Step 3: Make a Prediction\n",
        "\n",
        ">> Classification: assign the class that appears most frequently among the K neighbors (“majority vote” or “plurality vote”).\n",
        "\n",
        ">> Weighting schemes (e.g., giving more importance to closer neighbors) can be used for more nuanced decisions.\n",
        "\n",
        ">> Regression: predict a continuous output by averaging the target values of the K neighbors (unweighted or distance-weighted).\n",
        "\n",
        "> Step 4: Hyperparameter Tuning\n",
        "\n",
        "Choose a suitable K value:\n",
        "\n",
        ">> Small K (e.g., 1): more flexible but sensitive to noise (overfitting).\n",
        "\n",
        ">> Large K: smoother predictions but may ignore local patterns (underfitting).\n",
        "\n",
        "Cross-validation is commonly used to select an optimal K.\n",
        "\n",
        "| Task            | KNN Approach                                                                                                                     |\n",
        "| --------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Classification  | Majority vote among nearest neighbors; can weight by distance.           |\n",
        "| Regression      | Average (or weighted average) of neighbors' continuous target values.\n",
        "| Lazy learning   | No training—just storing data until prediction time.                                                   |\n",
        "| Distance metric | Euclidean, Manhattan, Minkowski, Hamming, and others depending on data.\n",
        "\n",
        "[1]: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?utm_source=chatgpt.com \"K-nearest neighbors algorithm\"\n",
        "[2]: https://www.ibm.com/think/topics/knn?utm_source=chatgpt.com \"What is the k-nearest neighbors algorithm?\"\n",
        "[3]: https://www.datacamp.com/tutorial/k-nearest-neighbors-knn-classification-with-r-tutorial?utm_source=chatgpt.com \"K-Nearest Neighbors (KNN) Classification with R Tutorial\"\n",
        "[4]: https://www.pinecone.io/learn/k-nearest-neighbor/?utm_source=chatgpt.com \"K-Nearest Neighbor (KNN) Explained\"\n",
        "[5]: https://www.geeksforgeeks.org/machine-learning/k-nearest-neighbors-knn-regression-with-scikit-learn/?utm_source=chatgpt.com \"K-Nearest Neighbors (KNN) Regression with Scikit-Learn\"\n",
        "[6]: https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/?utm_source=chatgpt.com \"KNN algorithm: Introduction to K-Nearest Neighbors\"\n",
        "[7]: https://www.grammarly.com/blog/ai/what-is-k-nearest-neighbors/?utm_source=chatgpt.com \"What Is K-Nearest Neighbors (KNNs) Algorithm?\"\n"
      ],
      "metadata": {
        "id": "IMv7QCYzO2Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "ANS:\n",
        "\n",
        "#What Is the Curse of Dimensionality?\n",
        "\n",
        "The term—coined by mathematician Richard E. Bellman—describes a group of issues that arise when working with high-dimensional data (i.e., datasets with many features).\n",
        "\n",
        "Key issues include:\n",
        "\n",
        "> Explosive growth of data space: As dimensions increase, the volume of the feature space grows exponentially, leading to extremely sparse data distributions.\n",
        "\n",
        "> Distance concentration: In high dimensions, distances between points tend to converge. The difference between the nearest and farthest neighbors becomes negligible, making distance measures less meaningful.\n",
        "\n",
        "> Data scarcity: To adequately sample a high-dimensional space, you need exponentially more data—otherwise patterns become unreliable.\n",
        "\n",
        "> Generalization challenges: With sparse data, models may overfit due to irrelevant or noisy features, hurting performance on unseen data.\n",
        "\n",
        "#How the Curse of Dimensionality Harms KNN?\n",
        "\n",
        "KNN heavily relies on distance to identify nearest neighbors. Here's how high dimensionality hinders its effectiveness:\n",
        "\n",
        "1. Distance loses meaning\n",
        "\n",
        ">> In high-dimensional space, nearly all distances converge, so it's hard to distinguish between truly close and far points. KNN may end up using arbitrary “neighbors,” leading to poor classification or regression results.\n",
        "\n",
        "2. Increased computational demand\n",
        "\n",
        ">> KNN must compute distances to every point in the training set. In high dimensions, each distance calculation becomes costlier, making the algorithm slow and computationally expensive.\n",
        "\n",
        "3. Amplified sparsity and neighbor scarcity\n",
        "\n",
        ">> With data spread thinly across the space, “neighbors” are less likely to be meaningfully similar, resulting in unreliable predictions.\n",
        "\n",
        "4. More data needed to perform well\n",
        "\n",
        ">> High-dimensionality demands significantly more data for KNN to generalize effectively. Without enough samples, the algorithm struggles with accuracy.\n",
        "\n",
        "#Curse of Dimensionality vs. KNN\n",
        "\n",
        "| Challenge                         | Effect on KNN Performance                                                          |\n",
        "| --------------------------------- | ---------------------------------------------------------------------------------- |\n",
        "| **Distance concentration**        | Neighbors become indistinguishable—distance loses its discriminative power         |\n",
        "| **High computational complexity** | Distance calculations become slow and resource-intensive                           |\n",
        "| **Data sparsity**                 | Reduced likelihood of finding genuinely similar neighbors—prediction quality drops |\n",
        "| **Exponential data needs**        | Requires much larger datasets to maintain performance—data-intensive               |\n"
      ],
      "metadata": {
        "id": "tfU-8dQ8QP-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3: What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "ANS:\n",
        "\n",
        "#What Is Principal Component Analysis (PCA)?\n",
        "\n",
        "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms original features into a new set of uncorrelated variables—called principal components—ranked by the amount of variance they capture in the data.\n",
        "\n",
        "> The first principal component is the direction (a linear combination of features) along which the variance of the projected data is maximized. Subsequent components capture the most remaining variance, subject to orthogonality constraints.\n",
        "\n",
        "> PCA is widely used for data exploration, visualization, and preprocessing to reduce dimensionality while preserving as much information as possible.\n",
        "\n",
        "In practice, PCA derives these components via eigen-decomposition of the covariance matrix or via singular value decomposition (SVD). It then projects high-dimensional data into a lower-dimensional space formed by the top principal components.\n",
        "\n",
        "#How Is PCA Different from Feature Selection?\n",
        "\n",
        "While both aim to reduce dimensionality, PCA and feature selection are fundamentally different in their approaches and outcomes:\n",
        "\n",
        "> Feature Selection involves picking a subset of the original features, retaining only those deemed most relevant for the task. It preserves interpretability, because the chosen features remain unchanged. → Examples include techniques like RFE, LASSO, mutual information, and tree-based importance.\n",
        "\n",
        "> PCA, on the other hand, is a feature extraction technique: it creates new features (components) that are linear combinations of the originals. These new features are often not interpretable or directly relatable to the original variables.\n",
        "\n",
        "#Key distinctions:\n",
        "\n",
        "| Aspect                                                                                                     | Feature Selection                    | PCA (Feature Extraction)                       |\n",
        "| ---------------------------------------------------------------------------------------------------------- | ------------------------------------ | ---------------------------------------------- |\n",
        "| Output                                                                                                     | Subset of original features          | New transformed components                     |\n",
        "| Interpretability                                                                                           | High—kept original features          | Low—components are combinations                |\n",
        "| Use of target info                                                                                         | Often uses target (supervised)       | Unsupervised (ignores target)                  |\n",
        "| Goal                                                                                                       | Remove irrelevant/redundant features | Capture maximum variance with fewer dimensions |\n",
        "|                                       |                                                |\n",
        "\n",
        "[1]: https://en.wikipedia.org/wiki/Feature_selection?utm_source=chatgpt.com \"Feature selection\"\n",
        "[2]: https://vinesmsuic.github.io/notes-ML5Selection/?utm_source=chatgpt.com \"Feature Selection and Dimensionality Reduction | Vines' Log\"\n",
        "[3]: https://dataheadhunters.com/academy/dimensionality-reduction-vs-feature-selection-simplifying-data/?utm_source=chatgpt.com \"Dimensionality Reduction vs Feature Selection: Simplifying Data\"\n",
        "[4]: https://blog.kxy.ai/5-reasons-you-should-never-use-pca-for-feature-selection/index.html?utm_source=chatgpt.com \"5 Reasons You Should Never Use PCA For Feature Selection\"\n"
      ],
      "metadata": {
        "id": "VaDTWqrRRaAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "ANS:\n",
        "\n",
        "#What Are Eigenvalues and Eigenvectors in PCA?\n",
        "\n",
        "Eigenvectors are special directions (vectors) in the feature space along which repeated application of a linear transformation (like multiplying by the covariance matrix) only scales the vector, not changes its orientation. Mathematically, for a matrix 𝐴 and vector 𝑣:\n",
        "\n",
        "(Java)\n",
        "\n",
        "A v = λ v\n",
        "\n",
        "> v is an eigenvector\n",
        "\n",
        "> 𝜆 is its corresponding eigenvalue, indicating the scaling factor or strength of that direction.\n",
        "\n",
        "In PCA, the covariance matrix of the data undergoes eigendecomposition, yielding several eigenvector-eigenvalue pairs:\n",
        "\n",
        "> Each eigenvector defines a principal component axis —i.e., a direction in which the data varies.\n",
        "\n",
        "> Each eigenvalue quantifies how much variance (spread) exists along that eigenvector direction.\n",
        "\n",
        "#Why Are They Important in PCA?\n",
        "\n",
        "1. Capturing Maximum Variance\n",
        "\n",
        ">> The first principal component is the eigenvector with the largest eigenvalue—it points in the direction where the data varies the most.\n",
        "\n",
        ">> Subsequent components are orthogonal (perpendicular) to earlier ones and capture remaining variance in descending order.\n",
        "\n",
        "2. Dimensionality Reduction & Data Compression\n",
        "\n",
        ">> By selecting only those eigenvectors whose eigenvalues are largest, PCA retains directions containing the most information.\n",
        "\n",
        ">> Discarding components with low eigenvalues reduces dimensionality while preserving most of the variance.\n",
        "\n",
        "3. Explained Variance\n",
        "\n",
        ">> The ratio of a component's eigenvalue to the total sum of eigenvalues indicates the proportion of variance explained.\n",
        "\n",
        ">> This helps determine how many components to keep—often visualized via a scree plot, where you look for an “elbow.”\n",
        "\n",
        "4. Interpreting Components via Loadings\n",
        "\n",
        ">> Loadings combine eigenvectors and eigenvalues to show how strongly each original variable contributes to each principal component.\n",
        "\n",
        "5. Optimal Orthogonal Projection\n",
        "\n",
        ">> PCA rotates data into a new coordinate system defined by the eigenvectors of the covariance matrix. In this transformed space, variance is aligned along the principal axes (eigenvectors), and the covariance matrix is diagonal.\n",
        "\n",
        "#Layman’s Analogy\n",
        "\n",
        "Imagine you have a scatter of points (data) in 2D space. PCA finds:\n",
        "\n",
        "> The longest direction where points stretch out (eigenvector 1 with largest eigenvalue).\n",
        "\n",
        "> Then the next direction perpendicular to that (eigenvector 2 with smaller eigenvalue).\n",
        "\n",
        "> These axes capture how the data spreads—highlighting the main structure and discarding noise.\n",
        "\n",
        "| Concept                      | Meaning in PCA                                                            |\n",
        "| ---------------------------- | ------------------------------------------------------------------------- |\n",
        "| **Eigenvector**              | Direction of a principal component—where variance is maximized            |\n",
        "| **Eigenvalue**               | Magnitude of variance along its eigenvector direction                     |\n",
        "| **First PC**                 | Eigenvector with the highest eigenvalue (direction of greatest spread)    |\n",
        "| **Component Selection**      | Choose components with largest eigenvalues to retain meaningful variance  |\n",
        "| **Loadings**                 | Combine eigenvectors and eigenvalues to understand variable contributions |\n",
        "| **Dimensionality Reduction** | Keep top components to compress data without losing much information      |\n"
      ],
      "metadata": {
        "id": "DDskEk9iVJvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "ANS:\n",
        "\n",
        "#Why PCA + KNN Makes Sense?\n",
        "\n",
        "1. Combatting the Curse of Dimensionality\n",
        "\n",
        ">> KNN heavily relies on distance metrics to find similar data points. In high-dimensional spaces, however, distances tend to become less meaningful—most points become nearly equidistant, making it tough to distinguish \"neighbors.\" PCA reduces the number of dimensions, preserving the most important variance and helping KNN work with more meaningful distances.\n",
        "\n",
        "2. Improved Computational Efficiency\n",
        "\n",
        ">> Reducing feature dimensions greatly speeds up KNN. Since distance computations scale with the number of features, PCA’s dimensionality reduction leads to faster neighbor lookups.\n",
        "\n",
        "3. Maintaining (or Improving) Accuracy\n",
        "\n",
        ">> In many real-world applications—like image recognition—PCA can help maintain or even enhance KNN’s accuracy by filtering out noise and focusing on the most informative directions.\n",
        "\n",
        ">> For example, in a study on MNIST digit classification, PCA drastically reduced dimensions while retaining high accuracy—leading to better computational efficiency without sacrificing performance.\n",
        "\n",
        "Similarly, for face recognition tasks, using PCA to extract features before applying KNN is a common and effective approach.\n",
        "\n",
        "#PCA + KNN Pipeline\n",
        "\n",
        "| Benefit                              | How PCA Helps KNN                                             |\n",
        "| ------------------------------------ | ------------------------------------------------------------- |\n",
        "| **Distance relevance**               | Reduces noise, making distances more meaningful               |\n",
        "| **Computational efficiency**         | Significantly speeds up distance calculations                 |\n",
        "| **Accuracy maintenance/improvement** | Filters out irrelevant features for better neighbor decisions |\n",
        "| **Noise reduction**                  | Removes redundant or uninformative dimensions                 |\n",
        "| **Better scalability**               | Makes KNN practical for high-dimensional datasets             |\n"
      ],
      "metadata": {
        "id": "yIS_cXM3WmK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "BmhLDRfaXU6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Pipeline\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# 1. KNN without scaling\n",
        "knn = KNeighborsClassifier()\n",
        "acc_unscaled = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "# 2. KNN with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "knn_scaled = KNeighborsClassifier()\n",
        "acc_scaled = cross_val_score(knn_scaled, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "# Optionally evaluate on test set:\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "test_acc_scaled = knn_scaled.score(scaler.transform(X_test), y_test)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n",
        "print(f\"Test accuracy with scaling: {test_acc_scaled:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnAdwAfpXyrk",
        "outputId": "1c34c804-2618-4729-f98f-b97d2e98f493"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.66\n",
            "Accuracy with scaling: 0.95\n",
            "Test accuracy with scaling: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7: : Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "omBfCybVYD0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load Wine dataset\n",
        "X, _ = load_wine(return_X_y=True)\n",
        "\n",
        "# 2. Standardize features (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Fit PCA using all available components (13 features in Wine dataset)\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Retrieve explained variance ratio for each principal component\n",
        "evr = pca.explained_variance_ratio_\n",
        "\n",
        "# 5. Print the results\n",
        "for idx, ratio in enumerate(evr, start=1):\n",
        "    print(f\"Principal Component {idx}: {ratio:.4f} ({ratio * 100:.2f}% of total variance)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OzGHXt7YKTy",
        "outputId": "a0742a5a-6a23-4de7-9cfa-58bcc820e630"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.3620 (36.20% of total variance)\n",
            "Principal Component 2: 0.1921 (19.21% of total variance)\n",
            "Principal Component 3: 0.1112 (11.12% of total variance)\n",
            "Principal Component 4: 0.0707 (7.07% of total variance)\n",
            "Principal Component 5: 0.0656 (6.56% of total variance)\n",
            "Principal Component 6: 0.0494 (4.94% of total variance)\n",
            "Principal Component 7: 0.0424 (4.24% of total variance)\n",
            "Principal Component 8: 0.0268 (2.68% of total variance)\n",
            "Principal Component 9: 0.0222 (2.22% of total variance)\n",
            "Principal Component 10: 0.0193 (1.93% of total variance)\n",
            "Principal Component 11: 0.0174 (1.74% of total variance)\n",
            "Principal Component 12: 0.0130 (1.30% of total variance)\n",
            "Principal Component 13: 0.0080 (0.80% of total variance)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n"
      ],
      "metadata": {
        "id": "r788MTy2YapG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Original dataset KNN\n",
        "knn = KNeighborsClassifier()\n",
        "acc_original = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "# PCA (top 2 components) then KNN\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier()\n",
        "acc_pca = cross_val_score(knn_pca, X_train_pca, y_train, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "print(f\"Original (scaled) accuracy: {acc_original:.2f}\")\n",
        "print(f\"PCA (2 components) accuracy: {acc_pca:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLWTWf0SYgyW",
        "outputId": "566901ce-ed72-4150-83a0-096cd578f317"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original (scaled) accuracy: 0.95\n",
            "PCA (2 components) accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "Krc19FdiYpB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Prepare data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Compare metrics\n",
        "results = {}\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(metric=metric, p=2 if metric=='euclidean' else 1)\n",
        "    acc = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy').mean()\n",
        "    results[metric] = acc\n",
        "\n",
        "print(\"Cross-validation accuracy:\")\n",
        "for metric, acc in results.items():\n",
        "    print(f\"  {metric.capitalize()}: {acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXxDs4GSYsNO",
        "outputId": "0b4dccd7-a531-4ed1-f676-35f6ba795917"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation accuracy:\n",
            "  Euclidean: 0.952\n",
            "  Manhattan: 0.968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "#Explain how you would:\n",
        "#● Use PCA to reduce dimensionality\n",
        "#● Decide how many components to keep\n",
        "#● Use KNN for classification post-dimensionality reduction\n",
        "#● Evaluate the model\n",
        "#● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "ANS:\n",
        "\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "\n",
        ">> Why PCA: PCA is a well-established linear dimensionality reduction technique frequently used in gene expression analysis to compress thousands of features (genes) into a lower-dimensional subspace, mitigating overfitting and computational noise.\n",
        "\n",
        "> Steps:\n",
        "\n",
        ">> Standardize the training data (zero mean, unit variance).\n",
        "\n",
        ">> Fit PCA on the training set only—then transform both training and test data using the fitted PCA. This avoids data leakage that would compromise validation.\n",
        "\n",
        ">> Work in the reduced space for downstream tasks, ensuring that distances (for KNN) are meaningful.\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "We recommend a multi-pronged approach:\n",
        "\n",
        ">> Explained-variance threshold: Select enough principal components to capture, say, 90–95% of variance. This is a standard heuristic in gene expression modeling.\n",
        "\n",
        ">> Cross-validated performance: Within inner cross-validation, treat the number of components as a hyperparameter. Choose the number that maximizes classifier performance (e.g., balanced accuracy, ROC-AUC).\n",
        "\n",
        ">> Statistical techniques (e.g., parallel/permutation analysis): Identify which PCs exceed what noise would produce—helpful when n ≪ p in maintaining signal-dominant dimensions.\n",
        "\n",
        "3. Use KNN for Classification After PCA\n",
        "\n",
        ">> Why KNN: Post-PCA, the feature space is more compact and denoised, allowing KNN to operate more effectively. KNN has been successfully applied in genomic classification contexts, especially after dimensionality reduction.\n",
        "\n",
        ">> Pipeline structure:\n",
        "\n",
        "Scaler → PCA (with chosen n components) → KNN (with tuned k and distance metric)\n",
        "\n",
        "Tune hyperparameters—particularly k and the distance metric (Euclidean or Manhattan)—within inner CV loops.\n",
        "\n",
        "4. Evaluate the Model Robustly\n",
        "\n",
        "Given the high-dimensional, small-sample nature of biomedical data, stringent evaluation is paramount:\n",
        "\n",
        ">> Nested cross-validation: Use an outer loop for unbiased performance estimation, and an inner loop for tuning PCA components and KNN hyperparameters. This structure prevents leakage and ensures trustworthy generalization estimates.\n",
        "Stratification: Use stratified folds to preserve class distribution in small datasets.\n",
        "\n",
        ">> Metrics: Beyond accuracy, report balanced accuracy, macro-ROC-AUC, macro-F1, and MCC, especially in class-imbalanced settings typical of cancer subtypes.\n",
        "\n",
        ">> Repetition and confidence intervals: Repeat CV runs and compute confidence intervals for performance to reflect inherent variability in small datasets.\n",
        "\n",
        ">> External validation (if available): Test on entirely independent cohorts or batches to assess robustness and generalizability.\n",
        "\n",
        "5. Justification for Stakeholders\n",
        "\n",
        "Here's how to articulate the strength of this pipeline:\n",
        "\n",
        "| Component                                | Justification                                                                                                                                                                      |\n",
        "| ---------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **PCA**                                  | Offers controlled dimensionality reduction, lowering overfitting risk when sample size is small. It’s widely used in genomics and bioinformatics. (\\[turn0search0])                |\n",
        "| **Hyperparameter tuning + Nested CV**    | Maintains statistical robustness and avoids over-optimistic performance—critical in clinical settings. (\\[turn0search4], \\[turn0search12])                                         |\n",
        "| **KNN**                                  | A transparent, interpretable classifier whose decision arises from actual observed neighbors—a boon for clinical interpretability. Success reported in similar biomedical domains. |\n",
        "| **Metrics and external validation**      | Ensure the model’s reliability across patient cohorts and highlight performance beyond simple accuracy—adding trust.                                                               |\n",
        "| **Pipeline reproducibility and clarity** | The entire process—scaling, PCA, KNN parameters—is clearly laid out and can be audited or redeployed across cohorts—aligns with clinical audit requirements.                       |\n",
        "\n"
      ],
      "metadata": {
        "id": "q0l5EQVmZsSw"
      }
    }
  ]
}